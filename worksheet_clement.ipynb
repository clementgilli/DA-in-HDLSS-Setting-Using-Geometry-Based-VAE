{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bc4f5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d8e408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(imgs):\n",
    "  '''\n",
    "  Display a batch of images (typically synthetic/generated images)\n",
    "  '''\n",
    "  r = 1\n",
    "  c = imgs.shape[0]\n",
    "  fig, axs = plt.subplots(r, c)\n",
    "  for j in range(c):\n",
    "    # black and white images\n",
    "    axs[j].imshow(imgs[j, 0,:,:].detach().cpu().numpy(), cmap='gray')\n",
    "    axs[j].axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0f09d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, multiplier=2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.multiplier = multiplier\n",
    "        self.fc1 = nn.Linear(input_dim, 400)\n",
    "        self.fc2 = nn.Linear(400, latent_dim * multiplier)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        if self.multiplier == 1:\n",
    "            x = x.view(batch_size, self.latent_dim)\n",
    "        else:\n",
    "          x = x.view(batch_size, self.latent_dim, self.multiplier)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.fc1 = nn.Linear(latent_dim, 400)\n",
    "        self.fc2 = nn.Linear(400, output_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        batch_size = z.size(0)\n",
    "        z = torch.relu(self.fc1(z))\n",
    "        z = self.fc2(z)\n",
    "        z = z.view(batch_size, 1, 28, 28)\n",
    "        return torch.sigmoid(z)\n",
    "\n",
    "class MetricNN(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, mode='diag'):\n",
    "        super(MetricNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 400)\n",
    "        self.mode = mode\n",
    "        if mode == 'diag':\n",
    "            self.fc2 = nn.Linear(400, latent_dim)\n",
    "        elif mode == 'lower':\n",
    "            self.fc2 = nn.Linear(400, latent_dim * (latent_dim - 1) // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "059e8de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_inverse(z, L, c, T, lambd):\n",
    "    s = 0\n",
    "    N = L.shape[1]\n",
    "    d = z.shape[1]\n",
    "    for i in range(N):\n",
    "        Li = L[i]\n",
    "        ci = c[i]\n",
    "        zi = z[i]\n",
    "        s += Li@Li.T * torch.exp (- torch.sum(zi - ci)**2 / T**2 ) + lambd*torch.eye(d, device=z.device)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RHVAEModel(nn.Module):\n",
    "    def __init__(self, input_dim=784, latent_dim=2, metric_mode = 'lower'):\n",
    "        super(RHVAEModel, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = Encoder(input_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, input_dim)\n",
    "        self.metric_nn = MetricNN(input_dim, latent_dim, mode=metric_mode)\n",
    "        self.G_inv = torch.eye(latent_dim)\n",
    "        self.G = torch.eye(latent_dim)\n",
    "        self.T_metric = 0.8\n",
    "        self.lambd_metric = 1e-3\n",
    "\n",
    "    def reparameterize(self, mean, logvar, mode='sample'):\n",
    "        \"\"\"\n",
    "        Samples from a normal distribution using the reparameterization trick.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mean : torch.Tensor\n",
    "            Mean of the normal distribution. Shape (batch_size, latent_dim)\n",
    "\n",
    "        logvar : torch.Tensor\n",
    "            Diagonal log variance of the normal distribution. Shape (batch_size,\n",
    "            latent_dim)\n",
    "\n",
    "        mode : 'sample' or 'mean'\n",
    "            Returns either a sample from qzx, or just the mean of qzx. The former\n",
    "            is useful at training time. The latter is useful at inference time as\n",
    "            the mean is usually used for reconstruction, rather than a sample.\n",
    "        \"\"\"\n",
    "        if mode=='sample':\n",
    "            # Implements the reparametrization trick (slide 43):\n",
    "            std = (0.5*logvar).exp()\n",
    "            eps = torch.randn(std.size()).to(std.device)\n",
    "            return mean + eps * std\n",
    "        elif mode=='mean':\n",
    "            return mean\n",
    "        else:\n",
    "            return ValueError(\"Unknown mode: {mode}\".format(mode))\n",
    "\n",
    "    def create_matrices(self, diags, lowers=None):\n",
    "        \"\"\"\n",
    "        Create the Cholesky factor L of the metric tensor G from the output of\n",
    "        the metric neural network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        diags : torch.Tensor\n",
    "            If using diagonal metric, the diagonal coefficients predicted by\n",
    "            the metric neural network. Shape (batch_size, latent_dim)\n",
    "\n",
    "        lowers : torch.Tensor\n",
    "            If using lower-triangular metric, the lower-triangular coefficients\n",
    "            predicted by the metric neural network. Shape\n",
    "            (batch_size, latent_dim * (latent_dim - 1) // 2)\n",
    "        \"\"\"\n",
    "        if self.metric_nn.mode == 'diag':\n",
    "            L = torch.diag_embed(torch.exp(0.5*diags))\n",
    "        elif self.metric_nn.mode == 'lower':\n",
    "            L = torch.zeros((diags.shape[0], self.latent_dim, self.latent_dim), device=diags.device)\n",
    "            tril_indices = torch.tril_indices(row=self.latent_dim, col=self.latent_dim, offset=-1)\n",
    "            L[:, tril_indices[0], tril_indices[1]] = lowers\n",
    "            diag_indices = torch.arange(self.latent_dim)\n",
    "            L[:, diag_indices, diag_indices] = diags\n",
    "        return L\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of model, used for training or reconstruction.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Batch of data. Shape (batch_size, n_chan, height, width)\n",
    "        \"\"\"\n",
    "\n",
    "        # stats_qzx is the output of the encoder\n",
    "        stats_qzx = self.encoder(x)\n",
    "\n",
    "        # Use the reparametrization trick to sample from q(z|x)\n",
    "        samples_qzx = self.reparameterize(*stats_qzx.unbind(-1), mode='sample')\n",
    "\n",
    "        # Decode the samples to image space\n",
    "        reconstructions = self.decoder(samples_qzx)\n",
    "\n",
    "        c = stats_qzx[:,:,0]  # Mean of q(z|x)\n",
    "        coefsL = self.metric_nn(x)\n",
    "        L = self.create_matrices(*coefsL.unbind(-1))\n",
    "        self.G_inv = G_inverse(samples_qzx, L, c, self.T_metric, self.lambd_metric)\n",
    "        self.G = torch.inverse(self.G_inv)\n",
    "\n",
    "        samples_vz = self.sample_vz(samples_qzx.shape[0])\n",
    "\n",
    "        # Return everything:\n",
    "        return {\n",
    "            'reconstructions': reconstructions,\n",
    "            'stats_qzx0': stats_qzx,\n",
    "            'samples_qzx0': samples_qzx,\n",
    "            'samples_vz0': samples_vz\n",
    "        }\n",
    "\n",
    "    def sample_qzx(self, x):\n",
    "        \"\"\"\n",
    "        Returns a sample z from the latent distribution q(z|x).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Batch of data. Shape (batch_size, n_chan, height, width)\n",
    "        \"\"\"\n",
    "        stats_qzx = self.encoder(x)\n",
    "        samples_qzx = self.reparameterize(*stats_qzx.unbind(-1))\n",
    "        return samples_qzx\n",
    "\n",
    "    def sample_vz(self, N):\n",
    "        return torch.distributions.multivariate_normal.MultivariateNormal(torch.zeros(self.G.shape[0]), covariance_matrix=self.G).sample((N,))\n",
    "\n",
    "    def sample_pz(self, N): \n",
    "        # A MODIFIER LA PRIOR\n",
    "        samples_pz = torch.randn(N, self.latent_dim, device=self.decoder.fc1.weight.device)\n",
    "        return samples_pz\n",
    "    \n",
    "    def leapfrog(self, z0, v0, epsilon, L):\n",
    "        raise NotImplementedError(\"Leapfrog method not implemented yet.\")\n",
    "\n",
    "    def generate_samples(self, N):\n",
    "        \n",
    "        samples_pz = self.sample_pz(N)\n",
    "\n",
    "        # Decode the z's to obtain samples in image space (here, probability\n",
    "        # maps which can later be sampled from or thresholded)\n",
    "        generations = self.decoder(samples_pz) # FILL IN CODE\n",
    "        return {'generations': generations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79556d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rhvae_loss(reconstructions, data, stats_qzx, beta=0.0012):\n",
    "    \"\"\"\n",
    "    Computes the VAE loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reconstructions : torch.Tensor\n",
    "        Reconstructed data. Shape (batch_size, n_chan, height, width)\n",
    "\n",
    "    data : torch.Tensor\n",
    "        Original data. Shape (batch_size, n_chan, height, width)\n",
    "\n",
    "    stats_qzx : torch.Tensor\n",
    "        Statistics of the latent distribution q(z|x). Shape\n",
    "        (batch_size, latent_dim * 2)\n",
    "    \"\"\"\n",
    "\n",
    "    # Reconstruction loss (binary cross-entropy)\n",
    "    bce_loss = nn.functional.binary_cross_entropy(\n",
    "        reconstructions,\n",
    "        data,\n",
    "        reduction='sum'\n",
    "    )\n",
    "\n",
    "    # KL divergence between q(z|x) and p(z) (closed form for two Gaussians)\n",
    "    mean, logvar = stats_qzx.unbind(-1)\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = bce_loss + beta * kl_div\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b551c001",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# MNIST Dataset\n",
    "mnist_trainset = datasets.MNIST(root='.data/mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_testset = datasets.MNIST(root='.data/mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "#create data loader with smaller dataset size\n",
    "max_mnist_size = 5000\n",
    "mnist_trainset_reduced = torch.utils.data.random_split(mnist_trainset, [max_mnist_size, len(mnist_trainset)-max_mnist_size])[0]\n",
    "mnist_train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "# download test dataset\n",
    "max_mnist_size = 1000\n",
    "mnist_testset_reduced = torch.utils.data.random_split(mnist_testset, [max_mnist_size, len(mnist_testset)-max_mnist_size])[0]\n",
    "mnist_test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=batch_size, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9bd5a71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "input_dim = 28 * 28\n",
    "latent_dim = 2\n",
    "\n",
    "learning_rate = 1e-3\n",
    "n_epoch = 10 # if running on GPU you can use more epochs (10 or more)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Model\n",
    "rhvae_model = RHVAEModel(input_dim=input_dim, latent_dim=latent_dim, metric_mode='diag')\n",
    "rhvae_model = rhvae_model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=rhvae_model.parameters(), lr=learning_rate, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a7ce1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 938/938 [00:05<00:00, 180.67batch/s, loss=5.18e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 11016.1782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 938/938 [00:05<00:00, 180.21batch/s, loss=4.96e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 9955.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 938/938 [00:05<00:00, 182.44batch/s, loss=5.24e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 9712.9525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 938/938 [00:05<00:00, 184.20batch/s, loss=5.24e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 9562.0301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 938/938 [00:05<00:00, 183.72batch/s, loss=4.38e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 9454.4064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 938/938 [00:05<00:00, 186.65batch/s, loss=4.45e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 9369.0356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 938/938 [00:05<00:00, 181.43batch/s, loss=4.7e+3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 9306.1004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 938/938 [00:05<00:00, 180.14batch/s, loss=4.26e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 9249.4194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 938/938 [00:05<00:00, 180.57batch/s, loss=5.26e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 9199.3869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 938/938 [00:05<00:00, 178.01batch/s, loss=4.33e+3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 9158.2653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rhvae_model.train()\n",
    "\n",
    "for epoch in range(0,n_epoch):\n",
    "  train_loss=0.0\n",
    "\n",
    "  with tqdm.tqdm(mnist_train_loader, unit=\"batch\") as tepoch:\n",
    "    for data, labels in tepoch:\n",
    "      tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "      # Put data on correct device, GPU or CPU\n",
    "      data = data.to(device)\n",
    "\n",
    "      # Pass the input data through the model\n",
    "      predict = rhvae_model(data)\n",
    "      reconstructions = predict['reconstructions']\n",
    "      stats_qzx0 = predict['stats_qzx0']\n",
    "      samples_qzx0 = predict['samples_qzx0']\n",
    "      samples_vz0 = predict['samples_vz0']\n",
    "\n",
    "      # Normalizing flows\n",
    "      #samples_zK, samples_vK = rhvae_model.leapfrog(samples_qzx0, samples_vz0, epsilon=0.1, L=10)\n",
    "\n",
    "      # Compute the RHVAE loss\n",
    "      loss = rhvae_loss(reconstructions, data, stats_qzx0)\n",
    "\n",
    "      # Backpropagate\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # Aggregate the training loss for display at the end of the epoch\n",
    "      train_loss += loss.item()\n",
    "\n",
    "      # tqdm bar displays the loss\n",
    "      tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "  print('Epoch {}: Train Loss: {:.4f}'.format(epoch, train_loss/len(mnist_train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f183e59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rhvae_model.eval()\n",
    "samples = rhvae_model.generate_samples(4)['generations'].detach().cpu()\n",
    "display_images(samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compstats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
